
# ollama-deployment-service.yaml
# Currectly NS and ports are hardcoded

# --- Deployment for Ollama ---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-nomic-embed-deployment # Name of the Deployment
  namespace: scdf # <--- ADDED: Specifies the namespace
  labels:
    app: ollama-nomic-embed      # Label for grouping resources
spec:
  replicas: 1 # Number of Ollama pods to run
  selector:
    matchLabels:
      app: ollama-nomic-embed # Selector to find pods managed by this Deployment
  template:
    metadata:
      labels:
        app: ollama-nomic-embed # Labels applied to the pods
      # Note: Pods inherit the namespace from their owning Deployment
    spec:
      containers:
      - name: ollama-container
        image: ollama/ollama:latest # Official Ollama Docker image
        ports:
        - containerPort: 11434 # Port Ollama listens on inside the container
        # --- Lifecycle hook to pull the model on startup ---
        lifecycle:
          postStart:
            exec:
              # Command to pull the nomic-embed-text model
              # This ensures the model is available after the container starts.
              # The container will start serving requests after this hook completes (if successful).
              # If the pull fails, the container might not start correctly or be marked as unhealthy
              # depending on further health checks (not defined here).
              command: ["/bin/sh", "-c", "ollama pull nomic-embed-text"]
        # --- Resource Requests and Limits (Recommended for production) ---
        # resources:
        #   requests:
        #     memory: "4Gi" # Adjust based on model size and usage
        #     cpu: "1"      # Adjust based on expected load
        #   limits:
        #     memory: "8Gi"
        #     cpu: "2"
        #
        # --- Environment Variables (Optional) ---
        # env:
        #   - name: OLLAMA_HOST
        #     value: "0.0.0.0" # Ensures Ollama binds to all interfaces if needed
        #   - name: OLLAMA_MODELS
        #     value: "/ollama/models" # If using a custom path for models with PVC
        #
        # --- Persistence for Ollama Models (Optional but Recommended) ---
        # To persist models across pod restarts, uncomment the following
        # and create a PersistentVolumeClaim named 'ollama-models-pvc' in the 'scdf' namespace.
        # volumeMounts:
        # - name: ollama-models-storage
        #   mountPath: /root/.ollama # Default Ollama models directory
      # volumes:
      # - name: ollama-models-storage
      #   persistentVolumeClaim:
      #     claimName: ollama-models-pvc # Name of your PersistentVolumeClaim (must also be in 'scdf' namespace)

---
# --- Service to Expose Ollama via NodePort ---
apiVersion: v1
kind: Service
metadata:
  name: ollama-nomic-embed-svc # Name of the Service
  namespace: scdf # <--- ADDED: Specifies the namespace
  labels:
    app: ollama-nomic-embed   # Label for grouping resources
spec:
  type: NodePort # Exposes the Service on each Node's IP at a static port
  selector:
    app: ollama-nomic-embed # Selects pods with this label (must match Deployment's pod labels)
  ports:
    - protocol: TCP
      port: 11434       # Port the Service listens on internally within the cluster
      targetPort: 11434 # Port on the Ollama pod to forward traffic to
      nodePort: 31434   # Static port exposed on each Kubernetes Node (30000-32767 range)
                        # Your Spring app will connect to <NodeIP>:31434
