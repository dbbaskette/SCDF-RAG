# Spring Cloud Data Flow on Kubernetes - Automated Installer

This project provides a fully automated shell script to deploy [Spring Cloud Data Flow (SCDF)](https://dataflow.spring.io/) on a local or cloud Kubernetes cluster, using RabbitMQ and MariaDB as backing services. The script handles all setup, error checking, and default application registration for a seamless developer experience.

---

## Features

- **One-command install**: Deploys SCDF, Skipper, RabbitMQ, and MariaDB via Helm.
- **MinIO S3 integration**: Easily deploy a MinIO S3-compatible server for object storage in the SCDF namespace.
- **Automatic cleanup**: Removes previous installations and verifies deletion before proceeding.
- **NodePort exposure**: All management UIs are available on your localhost for easy access.
- **Default app registration**: Downloads and registers the latest default RabbitMQ stream apps via the SCDF REST API.
- **Robust error handling**: Logs all actions and errors to the `logs/` directory for easy troubleshooting.
- **Fully commented and maintainable**: The scripts are easy to follow and modify.

---

## Prerequisites

- **Kubernetes cluster** (e.g., Docker Desktop, Minikube, Kind, or cloud provider)
- **kubectl** (configured to point to your cluster)
- **Helm** (v3+ recommended)
- **curl** and **bash**

---

## Usage

1. **Clone this repo** (or copy the scripts into your project):
    ```sh
    git clone <your-fork-or-this-repo-url>
    cd SCDF-RAG
    ```

2. **Make the scripts executable**:
    ```sh
    chmod +x scdf_install_k8s.sh minio_install_scdf.sh create_stream.sh
    ```

3. **Run the SCDF installer**:
    ```sh
    ./scdf_install_k8s.sh
    ```

4. **(Optional) Deploy MinIO S3 server**:
    ```sh
    ./minio_install_scdf.sh
    ```

5. **Deploy the PDF preprocessor stream**:
    ```sh
    ./create_stream.sh
    ```

6. **Access the management UIs:**
    - SCDF Dashboard: [http://127.0.0.1:30080/dashboard](http://127.0.0.1:30080/dashboard)
    - RabbitMQ UI: [http://127.0.0.1:31672](http://127.0.0.1:31672) (user/bitnami)
    - RabbitMQ AMQP: `localhost:30672` (user/bitnami)
    - MinIO Console: [http://127.0.0.1:9001](http://127.0.0.1:9001) (if enabled)

7. **Check logs and troubleshoot:**
    - All actions and errors are logged in the `logs/` directory.
    - If you encounter issues, check these logs for details.

---

## What the Scripts Do

- Cleans up any previous SCDF, RabbitMQ, and namespace resources, verifying deletion before continuing.
- Installs RabbitMQ via Helm and waits for readiness.
- Installs SCDF (with Skipper and MariaDB) via Helm and waits for readiness.
- Registers default stream apps as Docker images.
- Exposes all UIs/services on NodePorts for localhost access.
- Deploys a sample PDF preprocessor stream pipeline.
- Optionally deploys a MinIO S3-compatible server in the SCDF namespace for object storage.

---

## MinIO S3 Integration (Static hostPath)

This project uses a static PersistentVolume and PersistentVolumeClaim for MinIO, mapped to a host directory for reliable, local storage. This avoids dynamic provisioning issues and ensures data persists on your machine.

- **YAML location:** `resources/minio-pv-pvc.yaml` (auto-generated by the script)
- **Host path:** Set at runtime via `$MINIO_SOURCE_DIR` or defaults to `./sourceDocs` in your project root
- **Script:** `minio_install_scdf.sh` handles all cleanup, PV/PVC creation, and Helm install.

**How it works:**
1. Deletes any existing MinIO PVC/PV to avoid StorageClass mismatches.
2. Generates and applies the static PV/PVC YAML in `resources/`.
3. Installs MinIO via Helm, referencing the pre-created PVC (no storageClass set in Helm command).

**To deploy MinIO:**
```sh
./minio_install_scdf.sh
```

- To use a custom host path, run:
  ```sh
  MINIO_SOURCE_DIR=/your/custom/path ./minio_install_scdf.sh
  ```

---

## SCDF S3-to-Log Stream Setup

This project contains scripts and configuration to deploy a Spring Cloud Data Flow (SCDF) stream that reads files from an S3/MinIO bucket and writes them to a log sink, using RabbitMQ as the message broker.

### Files

- `create_stream.sh`  
  Bash script to create and deploy the SCDF stream. Reads all configuration from `create_stream.properties`.
- `create_stream.properties`  
  Properties file containing all configurable settings for S3/MinIO, RabbitMQ, and SCDF.

### Usage

1. **Edit `create_stream.properties`**
    - Set your S3/MinIO endpoint, bucket, and credentials.
    - Set your RabbitMQ host and credentials (e.g., `RABBIT_USER`, `RABBIT_PASS`).
    - Set SCDF connection details.
2. **Run the script:**
    ```sh
    ./create_stream.sh
    ```
    The script will:
    - Print debug info about the current configuration.
    - Destroy and recreate the stream in SCDF.
    - Bind all apps in the stream to the correct RabbitMQ service.
    - Log actions to `logs/create_stream.log`.

### Requirements
- Bash
- AWS CLI (for S3/MinIO testing)
- `spring-cloud-dataflow-shell.jar` (must be present in the working directory)
- Access to your SCDF and RabbitMQ services

### Notes
- All sensitive credentials are redacted in logs.
- To change any configuration, edit `create_stream.properties` and re-run the script.
- The script is designed to be easily portable for local or Kubernetes-based SCDF environments.

---

For more details, see comments at the top of `create_stream.sh`.

---

## Logs

All logs are written to the `logs/` directory for easier troubleshooting and organization:
- `logs/scdf_install_k8s.log`
- `logs/minio_install_scdf.log`
- `logs/create_stream.log`

---

## Customization

You can modify the scripts to change namespaces, image versions, stream definitions, and more. The scripts are fully commented for easy maintenance.

---

## License

MIT License

---

## Contributors

- [Your Name Here]

PRs and issues welcome!

---

## Uninstall / Cleanup

To remove all deployed resources:
```sh
kubectl delete namespace scdf
```
Or, rerun the scriptâ€”it will clean up before reinstalling.

---

## Troubleshooting

- If the SCDF dashboard, RabbitMQ UI, or MinIO Console is not accessible, ensure your Kubernetes cluster is running and NodePorts are not blocked by a firewall.
- Check the `logs/` directory for detailed error messages.
- If you see repeated 404s during app registration, the script will skip malformed entries and only register valid apps.

---

## References

- [Spring Cloud Data Flow Docs](https://dataflow.spring.io/docs/)
- [Bitnami SCDF Helm Chart](https://artifacthub.io/packages/helm/bitnami/spring-cloud-dataflow)
- [Bitnami RabbitMQ Helm Chart](https://artifacthub.io/packages/helm/bitnami/rabbitmq)
- [MinIO Helm Chart](https://artifacthub.io/packages/helm/minio/minio)
